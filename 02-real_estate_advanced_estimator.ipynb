{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some \"default\" libraries\n",
    "# You can now use Pandas to manipulate the Dataframe conveniently\n",
    "\n",
    "''' Data manipulation'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "''' Data visualization'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Real Estate - Advanced Estimator\n",
    "\n",
    "‚ùóÔ∏è In the previous challenge, we saw that if we have more flats than features in our dataset ($\\large n$ observations $\\large> p$ features) we can't \"solve\" the equation $\\large \\boldsymbol X \\cdot \\boldsymbol \\theta = \\boldsymbol y $. Without a deterministic formula for $\\large \\boldsymbol \\theta$, we would no longer be able to predict the prices of new flats!\n",
    "\n",
    "----\n",
    "\n",
    "üéØ In this exercise, we now have access to a bigger dataset consisting of 1000 flats, and we want to refine our prediction for the same new flat as before:\n",
    "\n",
    "- `Surface`: 3000 $ft^2$\n",
    "- `Bedrooms`: 5 \n",
    "- `Floors`: 1\n",
    "\n",
    "‚ùå Instead of solving $\\large \\boldsymbol X \\cdot \\boldsymbol \\theta = \\boldsymbol y $ with a matrix $\\large \\boldsymbol X$ of shape $ (1000,4)$ that is **`non-invertible`**...\n",
    "\n",
    "üöÄ ...we will find a $\\large {\\boldsymbol \\theta} = \\begin{bmatrix}\n",
    "     \\theta_0 \\\\\n",
    "     \\theta_1 \\\\\n",
    "    \\theta_2 \\\\\n",
    "     \\theta_3\n",
    "\\end{bmatrix}_{4 \\times 1}$ that minimizes the error $ \\large \\boldsymbol e = \\boldsymbol X \\cdot \\hat{\\boldsymbol \\theta} - \\boldsymbol y  $; this approach is called a **`Linear Regression model`**. We will measure this error $\\boldsymbol e$ using the Euclidean distance $\\large \\left\\|\\boldsymbol e\\right\\|$ and the **`Mean Squared Error.`**\n",
    "\n",
    "üëâ Let's compute $\\large \\hat{\\boldsymbol \\theta}$ to find an approximate estimation of the new flat's price.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Data Exploration\n",
    "\n",
    "We load the dataset `flats.csv` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/flats.csv')\n",
    "flats.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ Use `scatterplots` to visually figure out <u><i>which feature gives the most information about prices:</i></u>``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ It seems that `surface` is a stronger indicator of price than the number of bedrooms or floors. In statistics, we say that `price` is more **correlated** with `surface` than with other features. \n",
    "\n",
    "üëâ Let's double-check this by running [`pandas.DataFrame.corr`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) below, which computes correlation coefficients between each pair of columns of the DataFrame. \n",
    "\n",
    "<i> <u>Remarkable values:</u></i>\n",
    "* 1 means that the two columns are perfectly correlated üìà \n",
    "* -1 means that the two columns are perfectly inversely correlated üìâ \n",
    "* 0 means that the two columns are not *linearly* correlated üòê\n",
    "    \n",
    "<details>\n",
    "    <summary><i>Why do we use the correlation coefficient and not the covariance coefficient?</i></summary>\n",
    "\n",
    "‚úÖ <u>Similarities</u>:\n",
    "    \n",
    "- üìà Positive correlations and positive covariances between two variables X and Y mean the same thing: when X increases, Y increases, and vice-versa.\n",
    "- üìâ Negative correlations and negative covariances between two variables X and Y mean the same thing: when X increases, Y decreases, and vice-versa.\n",
    "- ü§î A null correlation and a null covariance between two variables X and Y mean the same thing: \n",
    "    - They are _not linearly correlated_ in the sense that there would exist two real numbers $a$ and $b$  such that $ Y = aX + b$ \n",
    "    - However, they could still have a different type of relation such as $Y = X^{2}$ (quadratic relation), $Y = e^{X}$ (exponential relation), $Y = ln(X)$ (logarithmic relation), $Y = sin(\\sqrt{1+X^7})$ (super weird relation), etc.\n",
    "\n",
    "‚ùóÔ∏è<u>Main differences</u>:\n",
    "\n",
    "- üòñ The covariance between two variables X and Y can be either infinitely positive or infinitely negative: $ cov(X, Y) \\in ( - \\infty ; + \\infty ) $\n",
    "    Example: if $ cov (X, Y) = 10 $ and $ cov (X, Z) = 30 $, can you say that X and Z are \"more correlated\"? _No_, because you cannot compare apples, oranges, and bananas.\n",
    "\n",
    "    üßëüèª‚Äçüè´ How to solve this problem? Consider the correlation instead, often denoted by the Greek Letter $\\rho$ (pronounce \"rh√¥\")\n",
    "\n",
    "üëâ <u>Consequence</u>:\n",
    "\n",
    "- You can view the correlation as a _standardized covariance_, we simply divide the covariance by both the standard deviation of $X$ and the standard deviation of $Y$    \n",
    "$$ \\large  \\rho(X,Y) = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y} \\in [0;1]$$\n",
    "\n",
    "Continuing our example: suppose now that $ \\rho(X, Y) = 0.80 $ and $ \\rho(X, Z) = 0.15 $, would you still want to say that X and Z are more correlated? No; in fact, X and Y are more correlated than X and Z!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé® For a quicker glimpse of this matrix, you can use a **heatmap** from [`seaborn.heatmap`](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n",
    "\n",
    "<details>\n",
    "    <summary><i>Additional tips to display a nicer correlation matrix</i></summary>\n",
    "\n",
    "- `cmap`: **Seaborn** being a visualization library built on top of **Matplotlib**, you can use the argument [`cmap`](https://matplotlib.org/stable/tutorials/colors/colormaps.html), which stands for _color map_\n",
    "- `annot`: to read the correlations even faster, you can show the correlation coefficients directly on the colored heatmap\n",
    "- `annot_kws`: you can customize how the correlation coefficients appear\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üß™ Test your code!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'flats',\n",
    "    shape=flats.shape,\n",
    "    columns=flats.columns\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Estimator with 1 Feature\n",
    "\n",
    "Let's try to build a statistical estimator of **price** as a function of only one feature: the **surface**.\n",
    "\n",
    "üéØ Let's try to fit a **linear regression** between the two variables.\n",
    "\n",
    "Practically speaking, we want to choose the best parameters $\\hat{\\boldsymbol \\theta}$ = (`slope`, `intercept`) such that the `predicted price = slope * surface + intercept` is as close as possible to the `price` in terms of Mean Squared Error (MSE).\n",
    "\n",
    "üìÖ During the next weeks, we will discover and study different models (Linear Regression, KNN, Logistic Regression, Neural Networks, etc.). \n",
    "\n",
    "üëâ An important part of your job is to choose the right model and optimize the parameters to make the best predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Visual Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìImplement the function `plot_line(slope, intercept)`\n",
    "\n",
    "When given the `slope` and `intercept` arguments, this function creates an array `predicted_price` and plots that array on top of the original (scattered) data.\n",
    "\n",
    "When you are done with coding the function, play with different values of `(slope, intercept)` until you find a ‚Äúgood linear approximation‚Äù of the data. Can you find the best fit?\n",
    "\n",
    "Visual guideline:\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/03-Maths/01-Algebra-Calculus/line_of_best_fit.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line(data, slope, intercept):\n",
    "    # First, we'll create x - an array of our independent variable values - for you.\n",
    "    x = data['surface']\n",
    "\n",
    "    # Now create an array of predicted prices using the predicted prices formula above\n",
    "    # Remember, we can do mathematical operations on an entire array at once!\n",
    "    y_pred = None\n",
    "    \n",
    "\n",
    "    # Finally, let's plot the line!\n",
    "    plt.plot(x, y_pred);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function by running the cell below, then change the values to see if you can approximate a good line of best fit!\n",
    "\n",
    "*Hint: you can run `plot_line()` multiple times in the same cell to get several lines on the same scatterplot* üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"surface\", y=\"price\", data=flats)\n",
    "plot_line(flats, 1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòÖ Not so easy (and not very ‚Äúscientific‚Äù), right?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Computational Approach\n",
    "\n",
    "üî• To make sure that our estimator line is the best possible one, we need to compute the **Mean Squared Error** between the **real prices** and the **predicted prices**!\n",
    "\n",
    "üëâ Remember that:\n",
    "- For each apartment, `predicted_price = slope * surface + intercept`\n",
    "- Both the **vector of real prices** and the **vector of predicted prices** are of shape $ (1000, 1)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.1) Squared Errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Step 1</b></u>\n",
    "\n",
    "‚ùìFor each row (_i.e. flat_), we should evaluate the `squared_error = (price - predicted_price)**2`‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_squared_errors(slope, intercept, surfaces, prices):\n",
    "    \"\"\"\n",
    "    TODO: return an array containing the squared errors between \n",
    "    all real prices from the dataset and the predicted prices\n",
    "    \"\"\"\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° A general principle in Data Science/Modeling is that $ \\large error = f(\\boldsymbol y, \\hat{\\boldsymbol y})$ where:\n",
    "\n",
    "- $ \\large \\boldsymbol y $ is the real value\n",
    "- $ \\large \\hat{\\boldsymbol y} $ the predicted valute\n",
    "- $ \\large f$ is often called a **Loss Function** or a **Cost Function** \n",
    "    - üìÜ See `Machine Learning I > Model Tuning`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.2) Mean Squared Errors (MSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Step 2</b></u>\n",
    "\n",
    "‚ùì Create the `mse` function which should return the mean of the array returned by the `squared_errors` function. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(slope, intercept, surfaces, prices):\n",
    "    '''TODO: Return the mean of the array contained in squared_errors as a float.'''\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ In section _(2.1) Visual approach_, you visually tried to estimate the \"best line\", which consists in finding the best pair `(slope, intercept)`. \n",
    "\n",
    "‚ùìUsing this \"best pair\", compute the MSE of your estimator. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Finding the Best Parameters\n",
    "\n",
    "üëâ Keep playing with different values for `slope` and `intercept` and try to get the best fit by hand!  Notice how hard it is to optimize both parameters at the same time.\n",
    "\n",
    "üëá Follow the steps below to get an idea of one potential approach:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.3.1) Finding the Best Slope"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i><u> Step 1 :</u></i></b>\n",
    "\n",
    "Start by fixing an  `initial_intercept` with your best estimate, then find a slope that approximately minimizes the function  `mse = f(slope)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an initial_intercept, for instance, we may suppose there is always a small transaction fee even for very small flats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of 100 slopes that contains what you believe is the optimal slope (hint: np.linspace())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of MSEs for each slope value\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìPlot MSEs vs. slopes. Do you see a minimum‚ùì\n",
    "\n",
    "üôÉ If not, try another range of slopes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCompute the minimum value of MSE for your `initial_intercept`, and the corresponding `slope_best` value‚ùì\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Hint</i></summary>\n",
    "    \n",
    "Here you can use Python's built-in `.min()` function, as well as the `List.index()` method\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.3.2) Finding the Best Intercept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i><u> Step 2 :</u></i></b>\n",
    "\n",
    "üî® Now, let's fix the slope to that `slope_best` value, then re-use the previous approach to find \"the\" `intercept_best`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of 100 intercepts that contains what you believe is the optimal intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of MSEs for each intercept value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSEs against intercepts. Do you see a minimum? If not, try another range of slopes!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCompute `mse_min`, the minimum value of MSEs when slope is equal to `slope_best`, and store the corresponding best intercept as `intercept_best`‚ùì\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_min = None\n",
    "intercept_best = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üß™ Test your code!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'univariate',\n",
    "    mse_min=mse_min,                \n",
    "    slope_best=slope_best,\n",
    "    intercept_best=intercept_best\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ **Great job**! By adding a non-zero intercept parameter, we were able to reduce the MSE even more (feel free to plot the regression line on your scatter plot to visually confirm the approximate fit).\n",
    "\n",
    "‚ùìHowever, what guarantees that these (`intercept_best`, `slope_best`) parameters are really the best ones? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üëÄ Explanations</summary>\n",
    "\n",
    "We could maybe find an even better slope value by repeating step ‚ë†, this time fixing the intercept at `intercept_best`, and then repeating step ‚ë° with the new slope to adjust the intercept again.\n",
    "\n",
    "To find the global minimum of a 2-parameter function `rmse = f(slope, intercept)`, we may have to repeat steps ‚ë† and ‚ë° indefinitely until values converge towards absolute minima - with no guarantee of success.\n",
    "\n",
    "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/decision-science/real-estate-minimizer.png'>\n",
    "\n",
    "üí™ You've just discovered one of the most fundamental aspects of Machine Learning: **the iterative process of finding minima**.\n",
    "\n",
    "üëâ As you might guess, in the world of Data Science, algorithms have been developed to automate and optimize such processes. In the next few weeks, you will discover the power of other algorithms such as **Gradient Descent**, and Python libraries such as `Statsmodels` that perform this iterative process for you.\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Run the cells below if you are curious and want to find the real best slope and intercept for this dataset, computed using **Gradient Descent** (üìÜ covered in **Machine Learning I > Under The Hood**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this out (Seaborn visual solution)\n",
    "sns.regplot(data=flats, x='surface', y='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this out (statsmodels, exact solution)!\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "regression = smf.ols(formula= 'price ~ surface', data=flats).fit()\n",
    "\n",
    "print(\"intercept_best\", regression.params['Intercept'])\n",
    "print(\"slope_best\", regression.params['surface'])\n",
    "print('mse_best: ', np.mean(regression.resid**2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìLet's go back to our initial question: what is your new prediction for the 5th flat below? How does it compare with your initial prediction based on only 4 flats?\n",
    "\n",
    "- `surface`: 3000 $ft^2$\n",
    "- `bedrooms`: 5 \n",
    "- `floors`: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predicted price (Remember that the real price is 750,000$)\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è It's better than our initial deterministic estimator based on only 4 flats, but obviously we are missing the information provided by the number of bedrooms and floors in this prediction!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Estimator with _All_ Features (`surface`, `bedrooms`, `floors`)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° A linear regression with three features (**Multivariate Linear Regression**) works the same way as with one feature, but instead of determining only 2 parameters to minimize the RMSE (`intercept` and `slope`), we will need to find 4 parameters: $\\hat{\\boldsymbol \\theta}$ = (`intercept`, `slope_surface`, `slope_bedrooms`, `slope_floors`).\n",
    "\n",
    "üóì There will be a lecture fully dedicated to **Multivariate Linear Regression** in the **Decision Science** module.\n",
    "\n",
    "üóì The same **Gradient Descent** iterative method is applicable, and you will code it yourself by hand later in the bootcamp.\n",
    "\n",
    "üëâ Meanwhile, feel free to run the cell below to see the final result of this multivariate regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the four regression coefficients by running this cell\n",
    "regression = smf.ols(formula= 'price ~ surface + bedrooms + floors', data=flats).fit()\n",
    "regression.params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ Now that we have found the best parameters $\\boldsymbol {\\hat \\theta}= \\begin{bmatrix}\n",
    "     \\theta_0 \\\\\n",
    "     \\theta_1 \\\\\n",
    "    \\theta_2 \\\\\n",
    "     \\theta_3 \\\\\n",
    "\\end{bmatrix}_{4 \\times 1} = \n",
    "\\begin{bmatrix}\n",
    "     \\theta_{intercept} \\\\\n",
    "     \\theta_{surface}\\\\\n",
    "    \\theta_{bedrooms} \\\\\n",
    "     \\theta_{floors}\n",
    "\\end{bmatrix}_{4 \\times 1} = \n",
    "\\begin{bmatrix}\n",
    "    18.154854 \\\\\n",
    "    0.286953 \\\\\n",
    "    -21.623564 \\\\\n",
    "    -3.811868\n",
    "\\end{bmatrix}_{4 \\times 1}\n",
    "$, \n",
    "\n",
    "we can predict the price of the new flat with:\n",
    "* $3000 ft^2$\n",
    "* $5$ bedrooms\n",
    "* located on the $1st$ floor\n",
    "\n",
    "$$ \\hat{y_5} = \\theta_0 + \\theta_1 \\times 3000 + \\theta_2 \\times 5 + \\theta_3 \\times 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the newly predicted price for the 5th flat? Is this prediction better?\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Concluding Remarks on Linear Algebra üß†"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **optimization problem** can be summarized as follows\n",
    "\n",
    "- We need to find a vector of parameters $\\hat{\\boldsymbol \\theta} = \\begin{bmatrix}\n",
    "     \\theta_{intercept} \\\\\n",
    "     \\theta_{surface}\\\\\n",
    "    \\theta_{bedrooms} \\\\\n",
    "     \\theta_{floors}\n",
    "\\end{bmatrix}_{4 \\times 1}$ \n",
    "\n",
    "- That minimizes an error $e = \\left\\|\\boldsymbol X\\cdot \\hat{\\boldsymbol \\theta} - \\boldsymbol y  \\right\\|^2$\n",
    "\n",
    "- For a given matrix of features $\\boldsymbol X$ [constant, surfaces, floors, bedrooms]  $\\begin{bmatrix}\n",
    "    1 & 620 & 1 & 1 \\\\\n",
    "    1 & 3280 & 4 & 2 \\\\\n",
    "    ... \\\\\n",
    "    1 & 1900 & 2 & 2 \\\\\n",
    "    1 & 1320 & 3 & 3\n",
    "   \\end{bmatrix}_{n \\times 4}\n",
    "$\n",
    "\n",
    "- and a vector of observations $\\boldsymbol y  = \\begin{bmatrix}\n",
    "           y_{1} \\\\\n",
    "           y_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           y_{1000}\n",
    "         \\end{bmatrix}$ (prices)\n",
    "\n",
    "Such a $\\large \\hat{\\theta}$ is reached when the \"derivatives\" of $\\boldsymbol e$, that is $ \\large 2 \\boldsymbol X^T\\cdot (\\boldsymbol X \\cdot \\hat{\\theta}‚àí\\boldsymbol y )$, equal zero (üëâ proof during the Decision Science module). \n",
    "    \n",
    "In other words, we need to solve the linear system $\\large (\\boldsymbol X^T\\cdot \\boldsymbol X)\\cdot \\hat{\\boldsymbol \\theta}=\\boldsymbol X^T \\cdot \\boldsymbol y $. \n",
    "    \n",
    "This linear system has a unique solution provided that no column of $\\boldsymbol X$ can be expressed as a linear combination of the others: in that case, $ \\large (\\boldsymbol X^T \\cdot \\boldsymbol X)^{-1}$ is invertible and the minimum is reached when $\\large \\hat{\\boldsymbol \\theta} = (\\boldsymbol X^T\\cdot \\boldsymbol X)^{-1} \\cdot \\boldsymbol X^T \\cdot \\boldsymbol y $. Notice how $\\boldsymbol X$ does not need to be squared anymore compared to the first challenge üí™.\n",
    "\n",
    "üí• However, keep in mind that **inverting matrices is computationally complex**. That is why other methods have been developed to find the minimum of a function, such as **Gradient Descent**.\n",
    "    \n",
    "üìö Read more on [Stats.StackExchange](https://stats.stackexchange.com/a/278779) if you are interested!\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations! \n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
